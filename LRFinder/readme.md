# LR Finder
`Learning rate` is a very important hyperparameter for `gradient` based optimization algorithms like `SGD` and variants for 
training deep neural networks successfully.  Set the learning too low and the training time becomes huge as the parameters update very slowly. Set it too high and the updates will skip over optimal solutions, or worse the optimizer might not converge at all!
